<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>毕设求生日志-S01E02</title>
    <link href="/2023/03/16/%E6%AF%95%E8%AE%BE%E6%B1%82%E7%94%9F%E6%97%A5%E5%BF%97-S01E02/"/>
    <url>/2023/03/16/%E6%AF%95%E8%AE%BE%E6%B1%82%E7%94%9F%E6%97%A5%E5%BF%97-S01E02/</url>
    
    <content type="html"><![CDATA[<h1 id="今日工作简表"><a href="#今日工作简表" class="headerlink" title="今日工作简表"></a>今日工作简表</h1><ul><li><p>看了王树森笔记的前几节，看到了高估问题</p></li><li><p>看了DataWhale的组队学习深度学习的预热直播</p></li></ul><h1 id="今日作息"><a href="#今日作息" class="headerlink" title="今日作息"></a>今日作息</h1><p>9：10起床的。虽然比昨天计划的七点多晚了一个多小时，但还好。其实七点多我被闹铃叫醒了，但因为昨天做了些锻炼，有点累，就再睡了会儿，感觉疲惫的一天会更糟糕。</p><p>反正，控制早起的根本不在于早起，而在于早睡。我今天一会儿上床再看会儿书，把自己搞困，十一点前正式入睡，带上眼罩，放上助眠音乐，到明天七点四十肯定已经睡够8h了，醒了直接就起，因为没有没睡够的理由了。</p><h1 id="大总结"><a href="#大总结" class="headerlink" title="大总结"></a>大总结</h1><h2 id="今日收获"><a href="#今日收获" class="headerlink" title="今日收获"></a>今日收获</h2><p>虽然距离昨天定下的任务还差很多，但还是有些收获的。</p><ul><li><p>重新回顾了价值学习、策略学习、Actor-Critic、SARSA、QLearning、多步TD、优先经验回放都是些什么。</p></li><li><p>了解了深度学习学习路径。不要再看吴恩达、李飞飞了，看李沐的视频就行，比较新。然后pytorch也可以跟着他的视频开始。后面有时间跟着小甲鱼系统过python，然后学数据分析四个包，然后是刘二大人的pytorch</p></li></ul><h2 id="今日困惑"><a href="#今日困惑" class="headerlink" title="今日困惑"></a>今日困惑</h2><ul><li><p>为什么训练要让数据尽量没有相关性，尽量独立分布</p></li><li><p>为什么价值学习会出现高估问题</p></li></ul><h2 id="仍需改进"><a href="#仍需改进" class="headerlink" title="仍需改进"></a>仍需改进</h2><ol><li><p>作息。早上没起来，导致也没晨跑，也没有足够时间完成学习任务。</p></li><li><p>玩手机的时间太长。以后学习日，控制只在吃水果、健身的时候刷刷B站，中午上床千万别玩。</p></li></ol><h2 id="列个计划"><a href="#列个计划" class="headerlink" title="列个计划"></a>列个计划</h2><p>明天：</p><ul><li><p>早期跑步</p></li><li><p>上午把党支部材料做完，交了</p></li><li><p>下午三点前把王树森的课程搞明白</p></li><li><p>下午五点前把视频做出来</p></li><li><p>晚上打听毕设，看一看学长的资料</p></li><li><p>最后把个人杂事给办了</p></li></ul><p>近期：</p><ul><li><p>这周末两天，家教外，把PPT做完</p></li><li><p>下周二结束前，把EASY RL这本书搞懂，在理论上彻底入门RL</p></li></ul><h1 id="闲扯几句"><a href="#闲扯几句" class="headerlink" title="闲扯几句"></a>闲扯几句</h1><p>别人都已经交初稿了！你一定要及其自律，提高效率，把进度赶上来！然后不要坚持特别稳的学习路线，要边学，边看论文了。</p>]]></content>
    
    
    <categories>
      
      <category>毕设求生纪实</category>
      
    </categories>
    
    
    <tags>
      
      <tag>日志、毕设、强化学习、20岁还没发过SCI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>王树森强化学习网课-L06-TD Learning</title>
    <link href="/2023/03/16/%E7%8E%8B%E6%A0%91%E6%A3%AE%E7%BD%91%E8%AF%BE-L06-%E7%BB%8F%E9%AA%8C%E5%9B%9E%E6%94%BE/"/>
    <url>/2023/03/16/%E7%8E%8B%E6%A0%91%E6%A3%AE%E7%BD%91%E8%AF%BE-L06-%E7%BB%8F%E9%AA%8C%E5%9B%9E%E6%94%BE/</url>
    
    <content type="html"><![CDATA[<p>这一节，继续讲价值训练中的tricks~</p><h1 id="Motivition"><a href="#Motivition" class="headerlink" title="Motivition"></a>Motivition</h1><p>此前DQN训练存在两个问题：</p><ol><li><p>每条经验（体现在每个transition（$s_t,a_t,r_t,s_{t+1}$）里）只被用了一次就抛弃了，他可能还没被发掘完。此之谓“经验浪费”</p></li><li><p>相邻transtition序列间的相关性太强</p><blockquote><p>解释一下：<br>相关性强对训练学习不利，为啥不利，我目前还真不懂。</p></blockquote></li></ol><p>于是有人提出了经验回放(Experience Replay)的策略。</p><h1 id="朴素经验回放"><a href="#朴素经验回放" class="headerlink" title="朴素经验回放"></a>朴素经验回放</h1><ol><li>开始我们创建一个FIFO队列结构的Experience Buffer，大小为$n$。$n$一般取很大，10W-100W间。</li><li>每次平均随机抽取m条经验序列(mini batch)，对他们分别进行求TD Error和随机梯度（SG:Stochastic Gradient）$g_{ti}$的操作。</li><li>对这些随机梯度求平均，得到平均梯度$g_t$，用它更新网络参数$\omega_{t+1} &#x3D; \omega_t - \alpha \cdot g_t$</li></ol><p>优点：</p><ol><li><p>随机从buffer中抽取，打乱了相邻序列的相关性</p></li><li><p>历史经验可能使用多次，加大了利用率</p><h1 id="优先经验回放"><a href="#优先经验回放" class="headerlink" title="优先经验回放"></a>优先经验回放</h1><p>随机抽取还不太科学。因为不同的经验，肯定重要性不同，重要性大的肯定应该加大被抽取的概率。<br><strong>怎么评判重要性呐？</strong><br>我们可以对buffer的每一条经验都求TD Error。TD Error越大，重要性越高。<br><strong>我们一般采取两种方案确定抽样概率</strong>$p_t$<strong>：</strong></p></li><li><p>$p_t\propto |\delta_t| + \epsilon$;$\epsilon$的作用是防止概率为0</p></li><li><p>$pt \propto \frac{1}{rank(\delta_t)}$；$rank(\delta_t)$就是对$\delta_t$的降序排列序号</p></li></ol><p>但非均匀抽样会导致偏差,所以需要相应地改变学习率(概率越大，学习率应该越小，让少学点他)。于是我们给原本的学习率都乘上一个系数:$(np_t)^{-\beta}$,$n$为被抽中次数,$\beta$为一个随学习过程变大的从0到1的参数</p><blockquote><p>问题<br>$n$保证被多次抽中的经验不被过多学习，$\beta$保证学习率始终缓慢降低。</p></blockquote><p>于是,优先经验回放的算法流程为:</p><ol><li>开始我们创建一个FIFO队列结构的Experience Buffer，大小为$N$,并且让每一个刚进buffer的经验的抽取概率都给到最大值。$N$一般取很大，10W-100W间。</li><li>根据TD Error更新每条经验的抽样概率，并根据概率分布随机抽取m条经验序列(mini batch)，更新他们的被学习次数，更新他们的学习率,求他们的随机梯度（SG:Stochastic Gradient）$g_{ti}$，最后求学习率与随机梯度之积。</li><li>对这些学习率和随机梯度之积取平均得$\alpha \cdot g_t$，用它更新网络参数$\omega_{t+1} &#x3D; \omega_t - \alpha \cdot g_t$</li></ol>]]></content>
    
    
    <categories>
      
      <category>强化学习入门</category>
      
    </categories>
    
    
    <tags>
      
      <tag>强化学习、课程笔记、王树森、20岁还没发过SCI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>王树森强化学习网课-L04-Actor-Critic Method</title>
    <link href="/2023/03/16/%E7%8E%8B%E6%A0%91%E6%A3%AE%E7%BD%91%E8%AF%BE-L04-Actor-Critic%20Method/"/>
    <url>/2023/03/16/%E7%8E%8B%E6%A0%91%E6%A3%AE%E7%BD%91%E8%AF%BE-L04-Actor-Critic%20Method/</url>
    
    <content type="html"><![CDATA[<p>第三课中，我们看到，策略学习的过程中，也需要估计动作价值函数。第二课的价值学习中，我们知道了怎么用DQN学习价值函数，而且明确了单纯用价值函数来选择动作，只能应付动作空间离散的情况。所以我们一般希望采用策略学习，并用价值网络估计价值，也就是将两者结合起来。他俩的结合就叫Actor-Critic Method，其中策略网络充当Actor，价值网络充当Critic。</p><h1 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h1><p><strong>因为整体算法逻辑和策略学习一致，我们就直接列写算法：</strong></p><ol><li><p>agent观测到$s_t$</p></li><li><p>agent根据当前策略网络$\pi(a_t|s_t;\theta_t)$抽样得到$a_t$</p></li><li><p>agent执行$a_t$，观测到$s_{t+1}$,$r_t$</p></li><li><p>agent根据TD Learning学习价值网络参数$\omega$:</p><blockquote><ol><li>$q_t &#x3D; q(s_t,a_t;\omega_t)$</li><li>$y_t &#x3D; r_t + \gamma \mathop{max}\limits_{a}q(s_{t+1},a;\omega_t)$</li><li>$\delta_t &#x3D; q_t - y_t$</li><li>$d_{\omega,t} &#x3D; \frac{\partial q(s_{t+1},a_t;\omega)}{\partial \omega}|_{\omega &#x3D; \omega_t}$</li><li>$\omega_{t+1} &#x3D; \omega_t - \alpha \cdot \delta_t \cdot d_{\omega,t}$</li></ol></blockquote></li><li><p>agent根据策略梯度学习策略网络参数$\theta$:</p><blockquote><ol><li>$d_{\theta,t} &#x3D; \frac{\partial log(\pi(a_t|s_t;\theta))}{\partial \theta}|_{\theta &#x3D; \theta_t}$</li><li>$\theta_{t+1} &#x3D; \theta_t + \beta \cdot q_t \cdot d_{\theta,t}$</li></ol></blockquote></li></ol><p>王树森在视频中还提出，$\theta_{t+1} &#x3D; \theta_t + \beta \cdot q_t \cdot d_{\theta,t}$这一步可以换成$\theta_{t+1} &#x3D; \theta_t + \beta \cdot \delta_t \cdot d_{\theta,t}$。其实就是，把$q_t$变成了$q_t - y_t$。这种形式叫做policy learning with baseline，$y_t$就是这个baseline。加上baseline，可以在期望不变的情况下，降低方差，加快收敛。具体原因在第九课介绍。<br>这里的价值网络估计的应该就是策略下动作价值函数。</p>]]></content>
    
    
    <categories>
      
      <category>强化学习入门</category>
      
    </categories>
    
    
    <tags>
      
      <tag>强化学习、课程笔记、王树森、20岁还没发过SCI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>王树森强化学习网课-L05-TD Learning</title>
    <link href="/2023/03/16/%E7%8E%8B%E6%A0%91%E6%A3%AE%E7%BD%91%E8%AF%BE-L05-TD%20Learning/"/>
    <url>/2023/03/16/%E7%8E%8B%E6%A0%91%E6%A3%AE%E7%BD%91%E8%AF%BE-L05-TD%20Learning/</url>
    
    <content type="html"><![CDATA[<p>在强化学习中，我们训练价值网络时，基本都会用到TD Learning的思想。比如在单纯的DQN中，我们学习用TD算法更新最优动作价值函数$Q^*(s_t,a_t)$或其神经网络$q(s_t,a_t;w_t)$参数；在Actor-Critic方法中，我们用TD算法更新策略下的动作价值函数$Q_{\pi}(s_t,a_t)$或其神经网络$q(s_t,a_t;w_t)$参数；在Alpho-Go中，他们用TD算法更新状态价值函数的神经网络参数$V_{\pi}(s_t)$。<br>这节课我们聚焦一下这个算法，介绍几个常见算法实现形式：SARSA，Q-Learning，Multi-Step TD Target。</p><blockquote><p>先都拿价值学习举例</p></blockquote><h1 id="SARSA"><a href="#SARSA" class="headerlink" title="SARSA"></a>SARSA</h1><h2 id="定义："><a href="#定义：" class="headerlink" title="定义："></a>定义：</h2><p>对策略下动作价值函数$Q_{\pi}(s_t,a_t)$进行学习的算法叫做SARSA算法。</p><blockquote><p>在Actor-Critic中，对策略下动作价值函数的神经网络参数的学习过程，我们就可以称为SARSA算法。</p></blockquote><h2 id="实现："><a href="#实现：" class="headerlink" title="实现："></a>实现：</h2><blockquote><p>进行SARSA核心部分（函数更新）的实现，需要得到5个数据：$s_t,a_t,r_t,s_{t+1},a_{t+1}$，这就是SARSA名字的由来（State-Action-Reward-State-Action）。</p></blockquote><p><strong>表格表示法（直接更新价值，价值学习）：</strong></p><blockquote><p>只能应对状态和动作空间都有限的情况，也就是有限离散。对于连续的也可以离散化。</p></blockquote><ol><li>最开始初始化一张策略$\pi$下的动作价值表：纵轴为状态空间，横轴为动作空间，每个元素代表一定状态一定动作下，在策略$\pi$下的动作价值。（我们要更新的就是这个表，在每轮的动作选择之前进行）</li><li>在$t$时刻学习时，根据策略$\pi$，抽样出拟进行动作$a_{t+1}$。（此时$s_t,a_t,r_t,s_{t+1},a_{t+1}$齐全了）</li><li>计算TD Target$y_t &#x3D; r_t + \gamma * Q_{\pi}(s_{t+1},a_{t+1})$。其中$Q_{\pi}(s_{t+1},a_{t+1})$就从动作价值表里直接读出来。</li><li>计算TD Error$\delta_t &#x3D; Q_{\pi}(s_t,a_t) - y_t$。其中$Q_{\pi}(s_t,a_t)$就从动作价值表里直接读出来。</li><li>更新动作价值表：$Q_{\pi}(s_t,a_t) &#x3D; Q_{\pi}(s_t,a_t) - \alpha \cdot \delta_t$</li><li>$t+1$时刻不用再选动作，直接沿用预测时的$a_{t+1}$</li></ol><p><strong>网络表示法（更新价值网络参数，进而改变值）：</strong></p><blockquote><p>这下状态可以连续了，但因为还是价值学习，动作还连续不了。</p></blockquote><ol><li><p>最开始初始化策略$\pi$下的动作价值函数网络$q_{\pi}(s,a;\omega)$的参数$\omega$。（我们要更新的就是这个$\omega$，在每轮的奖励获得之后、下一次动作选取之前进行）</p></li><li><p>在$t$时刻学习时，根据当前价值网络输出最大值，选出拟进行动作$a_{t+1}$。（此时$s_t,a_t,r_t,s_{t+1},a_{t+1}$齐全了）</p></li><li><p>计算TD Target$y_t &#x3D; r_t + \gamma * q_{\pi}(s_{t+1},a_{t+1};\omega_t)$。</p></li><li><p>计算TD Error$\delta_t &#x3D; q_{\pi}(s_t,a_t;\omega_t) - y_t$。</p></li><li><p>计算$d_{\omega,t} &#x3D; \frac{\partial q_{\pi}(s_t,a_t;\omega)}{\partial \omega}|_{\omega &#x3D; \omega_t}$</p></li><li><p>更新动作价值网络参数：$\omega_{t+1} &#x3D; \omega_t - \alpha \cdot \delta_t \cdot d_{\omega,t}$</p></li><li><p>$t+1$时刻不再选动作，直接沿用预测时的$a_{t+1}$</p><h1 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h1><h2 id="定义：-1"><a href="#定义：-1" class="headerlink" title="定义："></a>定义：</h2><p>对最优动作价值函数进行学习更新的算法叫做Q-Learning。</p><blockquote><p>DQN就是一种Q-Learning</p></blockquote></li></ol><h2 id="实现：-1"><a href="#实现：-1" class="headerlink" title="实现："></a>实现：</h2><p><strong>表格表示法：</strong></p><ol><li>最开始初始化一张动作价值表：纵轴为状态空间，横轴为动作空间，每个元素代表一定状态一定动作下，的最大动作价值。（我们要更新的就是这个表，在每轮的动作选择之前进行）</li><li>在$t$时刻学习时，根据动作价值表，选出最大动作价值最大的拟进行动作$a_{t+1}$，即$a_{t+1} &#x3D; arg\mathop{max}\limits_{a}Q^*(s_{t+1},a)$，也就是比较出$s_{t+1}$那一行中最大的动作$a$。（此时$s_t,a_t,r_t,s_{t+1},a_{t+1}$齐全了）</li><li>计算TD Target$y_t &#x3D; r_t + \gamma * Q^*(s_{t+1},a_{t+1})$。其中$Q^*(s_{t+1},a_{t+1})$查表可知。</li><li>计算TD Error$\delta_t &#x3D; Q^*(s_t,a_t) - y_t$。其中$Q^*(s_t,a_t)$查表可知。</li><li>更新动作价值表：$Q^*(s_t,a_t) &#x3D; Q^*(s_t,a_t) - \alpha \cdot \delta_t$</li><li>$t+1$时刻重新选择动作</li></ol><p><strong>网络表示法：</strong></p><ol><li><p>最开始初始化最优动作价值函数网络$q(s,a;\omega)$的参数$\omega$。（我们要更新的就是这个$\omega$，在每轮的动作选择之前进行）</p></li><li><p>每次选在动作$a_{t+1}$前，根据$a_{t+1} &#x3D; arg\mathop{max}\limits_{a}q(s_{t+1},a;\omega_t)$选定拟进行动作$a_{t+1}$。（此时$s_t,a_t,r_t,s_{t+1},a_{t+1}$齐全了）</p></li><li><p>计算TD Target$y_t &#x3D; r_t + \gamma * q(s_{t+1},a_{t+1};\omega_t)$。</p></li><li><p>计算TD Error$\delta_t &#x3D; q(s_t,a_t;\omega_t) - y_t$。</p></li><li><p>计算$d_{\omega,t} &#x3D; \frac{\partial q(s_t,a_t;\omega)}{\partial \omega}|_{\omega &#x3D; \omega_t}$</p></li><li><p>更新动作价值网络参数：$\omega_{t+1} &#x3D; \omega_t - \alpha \cdot \delta_t \cdot d_{\omega,t}$</p><h1 id="Multi-Step-TD-Target"><a href="#Multi-Step-TD-Target" class="headerlink" title="Multi-Step TD Target"></a>Multi-Step TD Target</h1><p>之前的TD算法，都是单步TD，我们在每个Transition后立刻进行更新，也就是一个transition我们更新一次，且只利用一次的真实奖励。我们可以考虑，每一次更新用上多次的真实奖励，这不就能让TD Target更加接近真实的Target吗？<br>于是我们对SARSA和Q-Learning可以有新的改进：</p><h2 id="SARSA-1"><a href="#SARSA-1" class="headerlink" title="SARSA"></a>SARSA</h2><p>$y_t &#x3D; \sum_{i&#x3D;0}^{m-1} \gamma^i \cdot r_i + \gamma^m \cdot Q_{\pi}(s_{t+m},a_{t+m})$或$y_t &#x3D; \sum_{i&#x3D;0}^{m-1} \gamma^i \cdot r_i + \gamma^m \cdot q_{\pi}(s_{t+m},a_{t+m};\omega_t)$</p><h2 id="Q-Learning-1"><a href="#Q-Learning-1" class="headerlink" title="Q-Learning"></a>Q-Learning</h2><p>$y_t &#x3D; \sum_{i&#x3D;0}^{m-1} \gamma^i \cdot r_i + \gamma^m \cdot Q^*(s_{t+m},a_{t+m})$或$y_t &#x3D; \sum_{i&#x3D;0}^{m-1} \gamma^i \cdot r_i + \gamma^m \cdot q(s_{t+m},a_{t+m};\omega_t)$<br>这样，我们一方面保证了一步一更新，另一方面提升了TD Target中含有的真实数据，可以取得更好的效果。Multi-Step就是改进TD算法的一个trik。</p><blockquote><p>非常类似滑动平均滤波</p></blockquote></li></ol>]]></content>
    
    
    <categories>
      
      <category>强化学习入门</category>
      
    </categories>
    
    
    <tags>
      
      <tag>强化学习、课程笔记、王树森、20岁还没发过SCI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>王树森强化学习网课-L03-策略学习</title>
    <link href="/2023/03/16/%E7%8E%8B%E6%A0%91%E6%A3%AE%E7%BD%91%E8%AF%BE-L03-%E7%AD%96%E7%95%A5%E5%AD%A6%E4%B9%A0/"/>
    <url>/2023/03/16/%E7%8E%8B%E6%A0%91%E6%A3%AE%E7%BD%91%E8%AF%BE-L03-%E7%AD%96%E7%95%A5%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<p>在第一课中,我们知道,实现强化学习任务可以通过价值学习或者策略学习.其中策略学习就是学习动作价值函数,并在每一轮中,选取动作价值最高的动作,这一点在第二课进行了详细阐述.<br>策略学习,则是直接学习策略函数$\pi(a_t|s_t)$,在第一课的结尾我曾猜想策略函数的学习应该是要基于策略下的状态价值函数$V_{\pi}(s_t) &#x3D; E_A[Q_{\pi}(s_t,A)]$.这里我知道猜对了哈哈.下面开始正经介绍.</p><h1 id="策略网络"><a href="#策略网络" class="headerlink" title="策略网络"></a>策略网络</h1><p>既然是对策略函数进行学习,自然是先构建一个策略函数的估计网络$\pi(a_t|s_t;\theta)$.我们要更新一个网络总要有个目标.巧了,我们在第一课谈过,如果我们在同一个状态state下,对不同策略$\pi$下的状态价值函数$V_{\pi}$进行比较,就知道哪个策略更能当前的状态赢面大了.那么如果我们针对state这个随机变量对$V_{\pi}(s)$求期望的话,就可以作为评价策略的依据了.如果这个期望越大,说明策略越好.</p><hr><p><strong>于是我们有这样的初步想法:</strong></p><blockquote><p>给定一个需要最大化的目标函数:<br>$J(\theta) &#x3D; E_S[V_{\pi}(S;\theta)]$<br>为什么上述的状态价值函数中含有策略函数网络的参数$\theta$呐?因为$V_{\pi}(S) &#x3D; \sum_a \pi(a|S) \cdot Q_{\pi}(S,a)$.现在$\pi(a|S)$变成了$\pi(a|S;\theta)$,$V_{\pi}(S)$自然变成了$V_{\pi}(S;\theta)$.<br>于是我们这样学习参数:<br>$\theta_{t+1} &#x3D; \theta_t + \beta \cdot \frac{\partial J(\theta)}{\partial \theta}|_{\theta &#x3D; \theta_t}$</p></blockquote><p>上述想法暂时无法实现,因为我们其实没法对state这个随机变量求期望.我们只能用当前值,假想它就是均值结果(实际上并不是,只是无偏估计),这种方法叫做蒙特卡洛法.</p><blockquote><p>解释下:<br>蒙特卡洛法求期望就是:对于一个随机变量,和这个变量的概率密度函数,正经的期望应该是随机变量和概率密度函数乘积的积分.但概率密度函数未知,我们就不数值计算了,我们看这个黑箱的概率密度函数的抽样结果是什么,它抽出来什么我们就认为均值是什么.在这里,这个未知的概率密度函数就是状态转换函数.策略学习中多次(3次)用到蒙特卡洛法,所以还是需要整明白的.</p></blockquote><hr><p><strong>于是我们用当前</strong>$s$<strong>的</strong>$V_{\pi}$<strong>,而不是所有</strong>$s$<strong>的</strong>$V_{\pi}$<strong>的均值,来作为价值函数网络参数</strong>$\theta$<strong>更新的标准:</strong></p><blockquote><p>$\theta_{t+1} &#x3D; \theta_t + \beta \cdot \frac{\partial V_{\pi}(s_t;\theta)}{\partial \theta}|<em>{\theta &#x3D; \theta_t}$<br>$\frac{\partial V</em>{\pi}(s_t;\theta)}{\partial \theta} &#x3D; \sum_a Q_{\pi}(s_t,a) \frac{\partial \pi(a|s_t;\theta)}{\partial \theta}|_{\theta &#x3D; \theta_t}$</p></blockquote><p>如果$a$是离散的,我们就按照上式对每个$a$求值再求和就行.但如果$a$是连续的,我们一般这种求和就会变成积分,如果积分项里有PDF,那就成了求期望,如果无法解析地求期望,我们可以用蒙特卡洛的思想近似期望.以下公式就是通过一个巧妙的变换,把求积分问题又变成了求期望问题,最后用蒙特卡洛近似了:<br>$\frac{\partial V_{\pi}(s_t;\theta)}{\partial \theta} &#x3D; \sum_a Q_{\pi}(s_t,a) \frac{\partial \pi(a|s_t;\theta)}{\partial \theta}|<em>{\theta &#x3D; \theta_t} &#x3D; \sum_a Q</em>{\pi}(s_t,a) \pi(a|s_t;\theta) \frac{\partial log(\pi(a|s_t;\theta))}{\partial \theta}|<em>{\theta &#x3D; \theta_t} &#x3D; E_A[Q</em>{\pi}(s_t,a) \frac{\partial log(\pi(a|s_t;\theta))}{\partial \theta}|<em>{\theta &#x3D; \theta_t}] &#x3D; Q</em>{\pi}(s_t,a) \frac{\partial log(\pi(a|s_t;\theta))}{\partial \theta}|_{\theta&#x3D; \theta_t}$</p><hr><p><strong>至此,我们有了训练策略网络的算法:</strong></p><blockquote><p>$q_t &#x3D; Q_{\pi}(s_t,a_t)$<br>$d_{\theta,t} &#x3D; \frac{\partial log(\pi(a|s_t;\theta))}{\partial \theta}|\theta &#x3D; \theta_t$<br>$g(a_t,\theta_t) &#x3D; q_t \cdot d_{\theta,t}$<br>$\theta_{t+1} &#x3D; \theta_t + \beta \cdot g(a_t,\theta_t)$</p></blockquote><p>好了,到此我们可以讨论$q_t$到底怎么来了.</p><ol><li>如果我们一整轮都不更新,然后跑完整个trial,我们就知道每一步的$q_t$了,这也是一步蒙特卡洛.这种方法称为REINFORCE方法.</li><li>我们也可以用价值学习中的价值网络来估计这个$q_t$,这种方法叫做Actor-Critic方法.</li></ol><p>单纯策略学习好像没有单纯价值学习好用.当然最好是直接走Actor-Critic方法,我们下一节介绍.</p>]]></content>
    
    
    <categories>
      
      <category>强化学习入门</category>
      
    </categories>
    
    
    <tags>
      
      <tag>强化学习、课程笔记、王树森、20岁还没发过SCI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>王树森强化学习网课-L02-价值学习</title>
    <link href="/2023/03/16/%E7%8E%8B%E6%A0%91%E6%A3%AE%E7%BD%91%E8%AF%BE-L02-%E4%BB%B7%E5%80%BC%E5%AD%A6%E4%B9%A0/"/>
    <url>/2023/03/16/%E7%8E%8B%E6%A0%91%E6%A3%AE%E7%BD%91%E8%AF%BE-L02-%E4%BB%B7%E5%80%BC%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h1 id="DQN与TD-Learning"><a href="#DQN与TD-Learning" class="headerlink" title="DQN与TD Learning"></a>DQN与TD Learning</h1><p>通过第一课，我们知道了什么是最优动作价值函数$Q^*$，也明白了其意义(在一定$s$下,$Q^*$给所有$a$打分,agent可据此选出得分最高的$a$)。过程如下:</p><blockquote><p>$Q^*(s_t,a_t) &#x3D; \mathop{max}\limits_{\pi}Q_{\pi}(s_t,a_t)$<br>$a_t &#x3D; \mathop{max}\limits_{a} Q^*(s_t,a)$</p></blockquote><p>看似非常美好,但有个根本性的问题:我们怎么得到$Q^*$?<br>实际上,通过学习$Q^*$来完成强化学习任务的方法就叫做价值学习.DQN就是用神经网络学习$Q^*$的价值学习方法.</p><h2 id="DQN"><a href="#DQN" class="headerlink" title="DQN"></a>DQN</h2><p><strong>DQN神经网络的形式如下:</strong></p><blockquote><p>$Q(s_t,a_t;\omega_t)$</p></blockquote><p>我们希望DQN的输出能够逼近真正的$Q^*$,就得学习更新网络参数$\omega_t$.因为我们的目标是使得神经网络的估计接近真实,也就是误差减小,自然地,我们会想到梯度下降法:</p><blockquote><p>$L_t &#x3D; \frac{1}{2} (Q^*(s_t,a_t) - Q(s_t,a_t;w_t))^2$<br>$\omega_{t+1} &#x3D; \omega_t - \alpha \cdot \frac{\partial L_t}{\partial \omega}|<em>{\omega &#x3D; \omega_t} &#x3D; \omega_t - \alpha \cdot (Q^*(s_t,a_t) - Q(s_t,a_t;w_t)) \cdot \frac{\partial Q(s_t,a_t;w)}{\partial \omega} |</em>{\omega &#x3D; \omega_t}$</p></blockquote><p>但这里还是用到了未知的$Q^*$,所以不可行.除非我们把所有的路径都遍历完,才能找出$Q^*$.真找着了,那还要强化学习干嘛?全部硬遍历,算到地老天荒吧.</p><hr><h2 id="TD-Learning"><a href="#TD-Learning" class="headerlink" title="TD Learning"></a>TD Learning</h2><p><strong>这时我们需要引入TD Learning的思想:</strong></p><ul><li>$Q(s_t,a_t;w_t)$是完全没有根据的预测(一步都没走),可信度最低,需要找一个可信度比他高的参考;</li><li>$Q^*(s_t,a_t)$是客观存在的结果,等于最好情况下的回报(所有步都走完了),可信度最高,但无法得到不能作为参考;</li><li>我们可不可以利用上需要更新DQN时已经获得的数据,得到一个可行度介于DQN和$Q^*$之间的参考目标?毕竟,我们需要更新DQN的$\omega_t$为$\omega_{t+1}$时,第$t$步已经走完了,这一步的奖励$r_t$和下一步的状态$s_{t+1}$已经得到了.我们可以发现:$r_t + \gamma \cdot Q(s_{t+1},a_{t+1};\omega_t)$就是一个介于DQN和$Q^*$的选择,我们称之为<strong>TD Target</strong>.</li></ul><hr><p><strong>具体怎么回事,可以看看这个不严谨的推理:</strong></p><ol><li><p>不管DQN还是$Q^*$,都是对回报$U_t$的估计,$U_t$根据定义满足下式:</p><blockquote><p>$U_t &#x3D; R_t + \gamma \cdot U_{t+1}$</p></blockquote></li><li><p>进而将$U_t$替换为DQN后可得到:</p><blockquote><p>$\begin{aligned}<br>Q(s_t,a_t;w_t)<br>&amp; \approx E[R_t + \gamma \cdot Q(s_{t+1},a_{t+1};w_t)] \<br>&amp; \approx r_t + \gamma \cdot Q(s_{t+1},a_{t+1};w_t) \<br>&amp; \approx r_t + \gamma \cdot \mathop{max}<em>\limits{a} Q(s</em>{t+1},a;w_t)<br>\end{aligned}$</p></blockquote></li></ol><hr><p><strong>好,总之我们目前有了</strong>$Q^*$<strong>的平替TD Target</strong>$y_t &#x3D; r_t + \gamma \cdot Q_{t+1}(s_{t+1},a_{t+1};w_t)$<strong>,就可以继续用梯度法更新权重了:</strong></p><blockquote><p>补充:<br>这里的$a_{t+1} &#x3D; arg\mathop{max}\limits_a Q(s_t{t+1},a;\omega_t)$.我们下一步的$a$不一定就是这个$a_{t+1}$,具体怎么回事后面再交代.</p></blockquote><blockquote><p>$\begin{aligned}<br>&amp; q_t &#x3D; Q(s_t,a_t;w_t)\<br>&amp; a_{t+1} &#x3D; arg\mathop{max}\limits_a Q(s_t{t+1},a;\omega_t) \<br>&amp; y_t &#x3D; r_t + \gamma \cdot Q(s_{t+1},a_{t+1};w_t) \<br>&amp; L_t &#x3D; \frac{1}{2} (q_t - y_t)^2 \<br>&amp; \omega_{t+1} &#x3D; \omega_t - \alpha \frac{\partial L_t}{\partial\omega}|<em>{\omega &#x3D; \omega_t} \<br>&amp; d_t &#x3D; \frac{\partial Q(s</em>{t+1},a_{t+1};w) }{\partial \omega}|<em>{\omega &#x3D; \omega_t} \<br>&amp; \omega</em>{t+1} &#x3D; \omega_t - \alpha (q_t-y_t) \cdot d_t \<br>\end{aligned}$</p></blockquote><p>或者说:</p><blockquote><p>$\begin{aligned}<br>&amp; q_t &#x3D; Q(s_t,a_t;w_t)\<br>&amp; a_{t+1} &#x3D; arg\mathop{max}\limits_a Q(s_t{t+1},a;\omega_t) \<br>&amp; y_t &#x3D; r_t + \gamma \cdot Q(s_{t+1},a_{t+1};w_t) \<br>&amp; d_t &#x3D; \frac{\partial Q(s_{t+1},a_{t+1};w) }{\partial \omega}|\omega &#x3D; \omega_t \<br>&amp; \omega_{t+1} &#x3D; \omega_t - \alpha (q_t-y_t) \cdot d_t \<br>\end{aligned}$</p></blockquote><p>这就是TD Learning的全部.你给DQN输入状态$s$,他就会给你返回所有动作的打分.然后你再选出其中分数最高的动作就完成了agent选动作的步骤.然后agent得到这一步的reward,感知到下一步的state时,就可以更新DQN了,更新完选出下一步的action,这就是下一轮循环了.</p><h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p>我们在计算TD Target的时候,用到了一个下一时刻动作的估计$a_{t+1} &#x3D; arg\mathop{max}\limits_a Q(s_t{t+1},a;\omega_t)$.这是我们站在$t$时刻对$t+1$时刻的估计,我们在$t+1$时刻,完全可以重新计算$a_{t+1} &#x3D; arg\mathop{max}\limits_a Q(s_t{t+1},a;\omega_{t+1})$.<br>如果我们真的采用了预测时给出的动作,这就死SARSA算法的思想,他的特点就是非常安全(毕竟你怎么预测就怎么做),相应地可能会表现的比较胆小.<br>如果我们用新的网络参数做了重新计算,那就是QLearning的思想,他的特点就是比较大胆,相应地没有SARSA那么安全.</p><h1 id="DQN案例"><a href="#DQN案例" class="headerlink" title="DQN案例"></a>DQN案例</h1><p>看演示,用DQN就能做打砖块的游戏了.但我不会用python搭神经网络.所以过完这个理论课后,我得赶紧跟着李沐学深度学习了.</p>]]></content>
    
    
    <categories>
      
      <category>强化学习入门</category>
      
    </categories>
    
    
    <tags>
      
      <tag>强化学习、课程笔记、王树森、20岁还没发过SCI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>王树森强化学习网课-L01-基本概念</title>
    <link href="/2023/03/16/%E7%8E%8B%E6%A0%91%E6%A3%AE%E7%BD%91%E8%AF%BE-L01-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
    <url>/2023/03/16/%E7%8E%8B%E6%A0%91%E6%A3%AE%E7%BD%91%E8%AF%BE-L01-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><blockquote><p>王树森老师的强化学习入门视频教程似乎是对我来说最清晰易懂的。他的视频我在上学期末看过一遍，大概知道了强化学习是干什么的，这个学期初又看了一遍，并且整理了这个笔记初版，基本了解了视频的所有内容，但还保留了些问题。这次我从整理笔记出发，把整个课程过一遍，真正形成自己的理解。笔记里的很多东西可能不是王老师的本意是我的误读，我的表述也会有很多不严谨的地方，本质来说就是我的一个小笔记。</p></blockquote><h1 id="第一课-基本概念"><a href="#第一课-基本概念" class="headerlink" title="第一课-基本概念"></a>第一课-基本概念</h1><blockquote><p>我们先拿一个室内机器人举例子。</p></blockquote><h2 id="强化学习世界观"><a href="#强化学习世界观" class="headerlink" title="强化学习世界观"></a>强化学习世界观</h2><p><strong>在强化学习眼里，是这么看待机器人的室内运动的：</strong><br>首先，我们把被研究对象和它所处的具体环境，提出来作为我们关注的系统，并把这两个主体分别称为agent和enviroment。比如我们把机器人叫做代理Agent，把室内环境叫做环境Enviroment。<br>agent可以感知到enviroment的状态state，然后根据这个state做出动作action。<br>enviroment根据agent的action，改变state。至此enviroment的工作结束。</p><blockquote><p>补充细节：<br>agent不一定能感知到完整的，没有噪音的state，一般只能感知到部分的，有噪音的state，称为observation<br>这个state不一定完全是描述enviroment的state，也可能包含agent的state，比如室内机器人的位置信息<br>agent存在的目的就是为按照我们的意愿改变state。</p></blockquote><p>除上述内容外，我们再给enviroment赋予一个给state变化打分reward的功能、给agent一个接收这个reward的功能。agent就是根据这个reward反馈来改进自己的。</p><hr><p><strong>我们在了解了agent，enviroment，action，state，reward这几个概念后，我们可以再把上面的过程描述一遍:</strong><br>此时刻的agent为了获得更大的reward,根据此时刻的state,做出此时刻的action;此时刻的enviroment根据预先定义好的规则,根据此时刻的state和action,更新出下一时刻的state,并给agent此时刻的reward;至此一个循环结束.<br>这个循环产生的’下一时刻的state’将被下一时刻的agent感知到,’此时刻的reward’可能会给下一时刻agent选择action提供参考.</p><hr><p><strong>看到这里应该明白:</strong><br>agent和enviroment交互的过程就是一个个循环的迭代,每一个循环包含四个环节.<br>分别是:</p><ol><li>agent根据当前状态state(此后称为$s_t$),在动作空间中选取一个动作action(此后称为$a_t$)执行.</li><li>enviroment根据当前状态$s_t$和当前agent动作$a_t$,进行状态更新得到$s_{t+1}$.</li><li>enviroment根据当前状态$s_t$和当前agent动作$a_t$,给agent奖励$r_t$.</li><li>agent根据历史信息，学习改变现有的选择action的策略（不一定有）</li></ol><hr><p><strong>前三个环节都有明确的输入输出,我们就可以把他们看作广义的函数,在强化学习中,我们将其称为:</strong></p><ol><li><p>策略函数$\pi(a|s) &#x3D; P(A&#x3D;a|S&#x3D;s)$;策略函数是一个概率分布函数(PDF),需要随机抽样得到真正执行的$a_t$.</p><blockquote><p>补充细节：<br>上述策略函数为随机策略，还有确定策略。但一般我们都用随机策略，保留一定的探索机会，可能跳出局部最优，而且随机策略更适合博弈。</p></blockquote></li><li><p>状态转移函数$p(s’|s,a)&#x3D;P(S’&#x3D;s’|S&#x3D;s,A&#x3D;a)$;状态转移函数也是一个PDF,需要随机抽样才能得到真正的状态更新$s_{t+1}$</p><blockquote><p>解释一下：<br>设计状态转移函数为概率分布函数感觉比较好解释,我们本来就无法对环境做精确的机理建模,当然应该允许概率的作用.</p></blockquote></li><li><p>enviroment给agent的奖励规则没有多做介绍，一般是个确定性的映射关系。</p><blockquote><p>发现问题：<br>是否可以考虑对奖励规则进行些理论分析，应该已经有人做这些工作了吧</p></blockquote></li></ol><p>如你所见,这三个环节中,策略函数和奖励规则是我们可以改变的.我们的研究就开始于此.</p><h2 id="强化学习核心思想"><a href="#强化学习核心思想" class="headerlink" title="强化学习核心思想"></a>强化学习核心思想</h2><p>对于强化学习,我个人的理解就是根据state选择合适的action,使得在任务结束时,agent获得的总reward最大.</p><blockquote><p>解释一下：<br>作为和监督&#x2F;无监督学习并行的机器学习一大分类，强化学习与监督学习的区别之一就是：没有明确的标签反馈告诉agent选择的action是否正确，agent获得的反馈只有状态转移造成的打分reward</p></blockquote><p><strong>首先,我们考虑我们评价策略policy或者动作action好坏的标准：</strong><br>我们知道，做一个action，就会有一个reward。那么我们是不是可以直接拿这个reward作为评判标准呐？</p><blockquote><p>这样做太短视了。我们都知道延迟满足的道理：我现在选择好好学习做好毕设，虽然现在有少许痛苦，但几个月后毕业时，我可以更轻松地玩耍（当前我的reward较小，但未来的reward会增长很多）。反过来，如果我现在选择随便浪，当前的reward可能稍微高点，但毕业时我的reward会狠狠地下跌。所以我们不仅要考虑当前的reward，还要考虑后面的reward。</p></blockquote><p>既然我们要通盘考虑，那能不能用整个试验trial的reward之和作为评判标准呐？</p><blockquote><p>这样做也不合适。因为在某个动作执行前的奖励，不是这个动作的结果。</p></blockquote><p>所以我们对t时刻的动作好坏的评判标准取为t时刻reward，及其以后所有reward的总和，我们称之为回报retrun：</p><blockquote><p>$U_t &#x3D; R_t + R_{t+1} + R_{t+2} + R_{t+3} + \cdots$</p></blockquote><p>在强化学习里,我们还不是直接用这个朴素的累计回报,而是用折扣累计回报:</p><blockquote><p>$U_t &#x3D; R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \gamma^3 R_{t+3} + \cdots$</p></blockquote><p>这里，我们把$\gamma$叫做折扣率，取值区间在$[0,1]$之间。之所以我们要引入折扣率，是因为一个动作一般对相邻时刻影响大，越往后影响越小。如果每次的奖励都是一个数量级，当前的奖励就很容易被未来的奖励冲淡，所以我们就对未来的奖励打一个折扣，越未来，折扣越狠。引入这么一个意义明确超参数就多一个调参自由度。<br>此后，我们谈及回报return，都是指以上定义。</p><hr><p><strong>于是，我们有了新问题，我们的评价标准——return包含了现在和未来时刻的奖励reward，这个是未知量啊！？</strong><br>我们要仔细剖析上面这句话究竟是什么意思：<br>首先，为什么说现在时刻奖励$R_t$是未知的呐？因为$R_t$是$s_t$和$a_t$的函数，虽然此时$s_t$已知，但$a_t$未知。但还好，我们就是根据$R_t$选择$a_t$的。如果$a_t$是有限的，理论上一个个带入就好了。所以$R_t$不是问题。<br>首先，未来的$s_t$和$a_t$是真不知道，所以$R_{t+N}$之类的，还是随机变量。<br>对于随机变量，我们如果知道它的概率分布，也可以通过求期望的方式得到估计值：</p><blockquote><p>$E[f(x)] &#x3D; \int_x p(x) f(x) dx$</p></blockquote><p>巧了，动作变量$a$的概率分布函数就是策略函数$\pi(a|s)$，状态变量的概率分布函数就是状态转移函数$p(s’|s,a)$。</p><blockquote><p>解释一下:<br>我们后面并不会真的求期望来求解.因为就算这两个PDF也不是全部已知的.只是这样的数学定义便于我们后面用别的方法实现.</p></blockquote><p><strong>总之，通过上面这一串步骤，我们得到了对回报函数的估计，也就是动作价值函数：</strong></p><blockquote><p>$Q_{\pi}(s_t,a_t) &#x3D; E[U_t|S_t &#x3D; s_t,A_t &#x3D; a_t]$</p></blockquote><p>当前回报函数中，已知量$s_t$被动作价值函数通过传入参数获得了；当前回报函数中，未知量$a_t$也被动作价值函数通过传入参数获得了；当前回报函数中，未知量$s_{t+1},s_{t+2},s_{t+3},\cdots,a_{t+1},a_{t+2},a_{t+3},\cdots$都在求期望的过程中消掉了。当然需要你提前提供一个策略函数$\pi(a|s)$。因此我认为这个动作价值函数应被称为策略$\pi$下的动作价值函数。<br>动作价值函数实际上有三个输入。除了参数列表的两个外，还有策略函数这个输入。就算当前的$s$和$a$都相同，策略$\pi$不同，动作价值也不一定一样。我们取相同的$s$和$a$下，使得$Q_{\pi}$最大的$\pi$,此时的$Q_{\pi}$反应了当前$s$下选择$a$所可能取得的最大return.</p><hr><p><strong>这个最优动作价值函数,就是真正的动作打分函数：</strong></p><blockquote><p>$Q^*(s_t,a_t) &#x3D; \mathop{max}\limits_{\pi}Q_{\pi}(s_t,a_t)$</p></blockquote><p>agent在一定的$s$下,只要选择$Q^*$最大的$a$就行了.到这里我们发现，有点偏题了。本来是要探讨怎么选择$\pi(a|s)$，结果探讨出来了可以根据$Q^*(s_t,a_t)$选择$a_t$。不过这也不亏，实际上强化学习的一种实现就是价值学习：$a_t &#x3D; \mathop{argmax}\limits_{a}Q^*(s_t,a)$，还有一种实现是策略学习，也就是我们本想探讨的用$\pi(a|s)$的抽样结果得到$a_t$。<br>评价策略函数的标准,就是状态价值函数,或者应该称为策略$\pi$下的状态价值函数：</p><blockquote><p>$V_{\pi}(s_t) &#x3D; E_A[Q_{\pi}(s_t,a_t)]$</p></blockquote><p>其实就是当前状态下，对所有此刻可能的动作价值函数求期望，也就是你这个状态平均能取得怎样的回报。<br>它的含义是,此策略下,到达当前状态$s$时,赢面有多大.如果我们固定$s$,分别求每个$\pi$下的$V_{\pi}$，就可以比较出$\pi$的好坏。具体怎么做,留到策略学习一讲.</p><h2 id="OpenAI-Gym的使用"><a href="#OpenAI-Gym的使用" class="headerlink" title="OpenAI Gym的使用"></a>OpenAI Gym的使用</h2><p>openAI Gym是强化学习的通用环境，非常好用。<br>网址为：<br><a href="http://gym.openai.com/">http://gym.openai.com/</a><br>当时安装gym碰到点问题。后来就是管理员权限打开Anaconda Prompt。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">pip install siwg<br>pip install gym[box2d]//这句我忘了对不对，反正运行第三句后，再运行脚本报错了这个，我就安了这个<br>pip install gym<br></code></pre></td></tr></table></figure><p>主要跟的这个链接做的（已点赞，感谢！）：<br><a href="https://blog.csdn.net/qq_34769201/article/details/95667042">https://blog.csdn.net/qq_34769201&#x2F;article&#x2F;details&#x2F;95667042</a></p>]]></content>
    
    
    <categories>
      
      <category>强化学习入门</category>
      
    </categories>
    
    
    <tags>
      
      <tag>强化学习、课程笔记、王树森、20岁还没发过SCI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>毕设求生日志-S01E01</title>
    <link href="/2023/03/15/%E6%AF%95%E8%AE%BE%E6%B1%82%E7%94%9F%E6%97%A5%E5%BF%97-S01E01/"/>
    <url>/2023/03/15/%E6%AF%95%E8%AE%BE%E6%B1%82%E7%94%9F%E6%97%A5%E5%BF%97-S01E01/</url>
    
    <content type="html"><![CDATA[<h1 id="今日工作简表"><a href="#今日工作简表" class="headerlink" title="今日工作简表"></a>今日工作简表</h1><ul><li><p>看了Easy-RL这本书的Tabular Methods和Policy Gradient两部分（看不太懂）</p></li><li><p>形策课看了《五万年中国简史》的中国的诞生章节与中华文明气质来源章节</p></li><li><p>把党支部的评议材料做了</p></li><li><p>接到一个新家教单子</p></li></ul><h1 id="今日作息"><a href="#今日作息" class="headerlink" title="今日作息"></a>今日作息</h1><p>醒来10：12，一觉醒来发现群友说四月就要交中期报告了，想到自己啥都没搞，很担心啊！！！这就是今天决定开始发博客记录毕设的原因。今天开始努力一定不完（😭）</p><h1 id="大总结"><a href="#大总结" class="headerlink" title="大总结"></a>大总结</h1><h2 id="今日收获"><a href="#今日收获" class="headerlink" title="今日收获"></a>今日收获</h2><p>可以说没啥收获哈哈。称得上的也就是知道了Sarasa和Qlearning区别，还有一些奇奇怪怪和主线关系不大的知识。主要是Easy-RL这个书的PDF太抽象了，我看王树森的视频当时挺清楚的，看这个感觉越看越迷糊。明天尽快过完，还是跟着王树森走吧。</p><h2 id="今日困惑"><a href="#今日困惑" class="headerlink" title="今日困惑"></a>今日困惑</h2><p>很多困惑，主要是他提供的代码怎么用没看懂。</p><h2 id="仍需改进"><a href="#仍需改进" class="headerlink" title="仍需改进"></a>仍需改进</h2><p>主要是作息。明儿一定得七点多起，洗漱一下八点多点，再跑个步，吃个早饭回来九点多开始工作，挺好。</p><h2 id="列个计划"><a href="#列个计划" class="headerlink" title="列个计划"></a>列个计划</h2><p>现在还很不清楚毕设的要求啥的，明天赶紧打听清楚，计划一下时间安排，定几个DDL。</p><p>明天暂定任务啊，就是先把王树森的网课笔记整理一下，输出一个视频《一口气回顾完RL网课》收录B站栏目《20岁还没发过SCI》。如果有时间再把Easy-RL快速翻完，虽然感觉浪费时间，但看一半的书总感觉很难受。</p><h1 id="闲扯几句"><a href="#闲扯几句" class="headerlink" title="闲扯几句"></a>闲扯几句</h1><p>你给我必须！必须！必须！坚持下来！每天记录毕设日志，督促自己学习！祝所有家人都能活过毕设。</p>]]></content>
    
    
    <categories>
      
      <category>毕设求生纪实</category>
      
    </categories>
    
    
    <tags>
      
      <tag>日志、毕设、强化学习、20岁还没发过SCI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2022/11/02/hello-world/"/>
    <url>/2022/11/02/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
